{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Armado de features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = 0\n",
    "window = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 24*3600*3\n",
    "\n",
    "time_windows = [('2019-04-18 00:00:00.000000', '2019-04-21 00:00:00.000000'), ('2019-04-19 00:00:00.000000', '2019-04-22 00:00:00.000000'), \n",
    "                ('2019-04-20 00:00:00.000000', '2019-04-23 00:00:00.000000'), ('2019-04-21 00:00:00.000000', '2019-04-24 00:00:00.000000'), \n",
    "                ('2019-04-24 00:00:00.000000', '2019-04-27 00:00:00.000000')]\n",
    "time_labels = [('2019-04-21 00:00:00.000000', '2019-04-24 00:00:00.000000'), ('2019-04-22 00:00:00.000000', '2019-04-25 00:00:00.000000'), \n",
    "               ('2019-04-23 00:00:00.000000', '2019-04-26 00:00:00.000000'), ('2019-04-24 00:00:00.000000', '2019-04-27 00:00:00.000000')]\n",
    "days = [18, 19, 20, 21, 24]\n",
    "\n",
    "model = pd.DataFrame()\n",
    "targets = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armadoFeatures(window):\n",
    "    auctions = pd.read_pickle('data/auctions_w'+str(window)+'.pkl')\n",
    "    gb = auctions.groupby('ref_hash')\n",
    "    auctions['n_auctions'] = gb['date'].transform('count')\n",
    "    auctions['last_auction'] = gb['date'].transform('max')\n",
    "    auctions['first_auction'] = gb['date'].transform('min')\n",
    "    auctions['diff_auctions'] = (auctions['last_auction'] - auctions['first_auction']).dt.total_seconds()\n",
    "    auctions['mean_time_auction'] = 0\n",
    "    auctions.loc[auctions['n_auctions'] > 1, 'mean_time_auction'] = ((auctions['last_auction'] - auctions['first_auction'])/ \\\n",
    "                                     (auctions['n_auctions'] -1)).dt.total_seconds()\n",
    "    auctions['first_auction_sec'] = (auctions['first_auction'] - pd.Timestamp(time_windows[window-1][0])).dt.total_seconds()\n",
    "    auctions['last_auction_sec'] = (auctions['last_auction'] - pd.Timestamp(time_windows[window-1][0])).dt.total_seconds()\n",
    "    auctions['last_auction_sec_to_end'] = (pd.Timestamp(time_windows[window-1][1]) - auctions['last_auction']).dt.total_seconds()\n",
    "    auctions['ref_type_id_1'] = auctions['ref_type_id'].apply(lambda x: 1 if x==1 else 0)\n",
    "    auctions['ref_type_id_7'] = auctions['ref_type_id'].apply(lambda x: 1 if x==7 else 0)\n",
    "    auctions['day'] = (auctions['date'].dt.day) - days[window-1]\n",
    "    gb = auctions.groupby('ref_hash')\n",
    "    auctions = auctions.join(gb['source_id'].value_counts().unstack().add_prefix('source_'), on='ref_hash')\n",
    "    auctions = pd.get_dummies(auctions, columns=['day'])\n",
    "    gb = auctions.groupby('ref_hash')\n",
    "    auctions['day_0'] = gb['day_0'].transform('sum')\n",
    "    auctions['day_1'] = gb['day_1'].transform('sum')\n",
    "    auctions['day_2'] = gb['day_2'].transform('sum')\n",
    "\n",
    "    auctions.drop_duplicates(subset='ref_hash', inplace=True)\n",
    "    auctions.drop(columns=['date', 'ref_type_id', 'last_auction', 'first_auction', 'source_id'], inplace=True)\n",
    "    auctions = auctions.fillna(0)\n",
    "\n",
    "    clicks = pd.read_pickle('data/clicks_w'+str(window)+'.pkl')\n",
    "    clicks.drop(columns=['action_id', 'agent_device'])\n",
    "    gb = clicks.groupby('ref_hash')\n",
    "    clicks['n_clicks'] = gb['created'].transform('count')\n",
    "    clicks['last_click'] = gb['created'].transform('max')\n",
    "    clicks['first_click'] = gb['created'].transform('min')\n",
    "    clicks['diff_clicks'] = (clicks['last_click'] - clicks['first_click']).dt.total_seconds()\n",
    "    clicks['mean_time_click'] = 0\n",
    "    clicks.loc[clicks['n_clicks'] > 1, 'mean_time_click'] = ((clicks['last_click'] - clicks['first_click'])/ \\\n",
    "                                     (clicks['n_clicks'] -1)).dt.total_seconds()\n",
    "    clicks['first_click_sec'] = (clicks['first_click'] - pd.Timestamp(time_windows[window-1][0])).dt.total_seconds()\n",
    "    clicks['last_click_sec'] = (clicks['last_click'] - pd.Timestamp(time_windows[window-1][0])).dt.total_seconds()\n",
    "    clicks['wifi_connection'] = clicks['wifi_connection'].map({True: 1, False: 0})\n",
    "    clicks['timeToClick_mean'] = clicks.groupby('ref_hash')['timeToClick'].transform('mean')\n",
    "    clicks.loc[clicks.touchX == 'Infinity', 'touchX'] = 1\n",
    "    clicks.loc[clicks.touchY == 'Infinity', 'touchY'] = 10\n",
    "    clicks[\"touchX\"] = pd.to_numeric(clicks[\"touchX\"])\n",
    "    clicks[\"touchY\"] = pd.to_numeric(clicks[\"touchY\"])\n",
    "    clicks['touch_bottom'] = clicks['touchY'].apply(lambda x: 1 if x<=1 else 0)\n",
    "    clicks['touch_bottom2'] = clicks['touchY'].apply(lambda x: 1 if x>1 and x<=2 else 0)\n",
    "    top_10_carrier_id = clicks['carrier_id'].value_counts().head(10)\n",
    "    clicks.loc[(~clicks['carrier_id'].isin(top_10_carrier_id.index))&(clicks['carrier_id'].notnull()), 'carrier_id'] = 'Other'\n",
    "    gb = clicks.groupby('ref_hash')\n",
    "    clicks = clicks.join(gb['advertiser_id'].value_counts().unstack().add_prefix('advertiser_id_'), on='ref_hash')\n",
    "    clicks = clicks.join(gb['source_id'].value_counts().unstack().add_prefix('source_id_'), on='ref_hash')\n",
    "    clicks['touchX_mean'] = gb['touchX'].transform('mean')\n",
    "    clicks['touchY_mean'] = gb['touchY'].transform('mean')\n",
    "    clicks['touchs_in_bottom'] = gb['touch_bottom'].transform('sum')\n",
    "    clicks['touchs_in_bottom2'] = gb['touch_bottom2'].transform('sum')\n",
    "    clicks['latitude_mean'] = gb['latitude'].transform('mean')\n",
    "    clicks['longitude_mean'] = gb['longitude'].transform('mean')\n",
    "    clicks['timeToClick_mean'] = clicks['timeToClick_mean'].fillna(clicks['timeToClick_mean'].mean())\n",
    "    clicks['touchX_mean'] = clicks['touchX_mean'].fillna(clicks['touchX_mean'].mean())\n",
    "    clicks['touchY_mean'] = clicks['touchY_mean'].fillna(clicks['touchY_mean'].mean())\n",
    "\n",
    "    clicks.drop_duplicates(subset='ref_hash', inplace=True)\n",
    "    clicks = clicks.drop(columns=['advertiser_id', 'source_id', 'created', 'country_code', 'latitude', 'longitude', 'wifi_connection', 'carrier_id', 'trans_id', 'agent_device',\n",
    "       'os_minor', 'os_major', 'specs_brand', 'brand', 'timeToClick', 'touchX', 'touchY', 'ref_type', 'last_click', 'first_click', 'touch_bottom', 'touch_bottom2', 'action_id'])\n",
    "\n",
    "    modelo = pd.merge(auctions, clicks, on='ref_hash', how='outer')\n",
    "    auctions = 0\n",
    "    clicks = 0\n",
    "    gc.collect()\n",
    "    \n",
    "    installs = pd.read_pickle('data/installs_w'+str(window)+'.pkl')\n",
    "    installs = installs.drop(columns=['device_countrycode', 'ip_address', 'event_uuid', 'click_hash', 'device_brand', 'device_model'])\n",
    "    #installs = installs.drop(columns=['device_language', 'device_countrycode', 'ip_address', 'event_uuid', 'device_brand', 'device_model', 'click_hash', 'session_user_agent'])\n",
    "    installs.loc[installs['kind'] == 'OPEN', 'kind'] = 'Open'\n",
    "    installs.loc[installs['kind'] == 'app open', 'kind'] = 'app_open'\n",
    "    installs.loc[installs['kind'] == 'af app open', 'kind'] = 'af_app_opened'\n",
    "    installs.loc[installs['kind'] == 'af_app_opend', 'kind'] = 'af_app_opened'\n",
    "    installs.loc[installs['kind'] == 'Session Begin', 'kind'] = 'sessionbegin'\n",
    "    installs.loc[installs['kind'] == 'signed in', 'kind'] = 'Sign In'\n",
    "    top_20_installs_kind = installs['kind'].value_counts().head(20)\n",
    "    installs.loc[(~installs['kind'].isin(top_20_installs_kind.index))&(installs['kind'].notnull()), 'kind'] = 'Other'\n",
    "    top_15_installs_app = installs['application_id'].value_counts().head(15)\n",
    "    installs.loc[(~installs['application_id'].isin(top_15_installs_app.index))&(installs['application_id'].notnull()), 'application_id'] = 'Other'\n",
    "    installs['created'] = pd.to_datetime(installs['created'], format='%Y-%m-%d %H:%M:%S')\n",
    "    installs['wifi_installs'] = installs['wifi'].map({True: 1, False: 0})\n",
    "    installs['attributed'] = installs['attributed'].map({True: 1, False: 0})\n",
    "    installs['implicit'] = installs['implicit'].map({True: 1, False: 0})\n",
    "    gb = installs.groupby('ref_hash')\n",
    "    installs = installs.join(gb['kind'].value_counts().unstack().add_prefix('kind_'), on='ref_hash')\n",
    "    installs = installs.join(gb['application_id'].value_counts().unstack().add_prefix('application_id_'), on='ref_hash')\n",
    "    installs['wifi_installs_mean'] = gb['wifi_installs'].transform('mean')\n",
    "    installs['attributed_installs_mean'] = gb['attributed'].transform('mean')\n",
    "    installs['n_installs'] = gb['created'].transform('count')\n",
    "    installs['last_install'] = gb['created'].transform('max')\n",
    "    installs['first_install'] = gb['created'].transform('min')\n",
    "    installs['diff_installs'] = (installs['last_install'] - installs['first_install']).dt.total_seconds()\n",
    "    installs['mean_time_install'] = 0\n",
    "    installs.loc[installs['n_installs'] > 1, 'mean_time_install'] = ((installs['last_install'] - installs['first_install'])/ \\\n",
    "                                     (installs['n_installs'] -1)).dt.total_seconds()\n",
    "    installs['first_install_sec'] = (installs['first_install'] - pd.Timestamp(time_windows[window-1][0])).dt.total_seconds()\n",
    "    installs['last_install_sec'] = (installs['last_install'] - pd.Timestamp(time_windows[window-1][0])).dt.total_seconds()\n",
    "    \n",
    "    installs.drop_duplicates(subset='ref_hash', inplace=True)\n",
    "    installs.drop(columns=['created', 'application_id', 'ref_type', 'attributed',\n",
    "       'implicit', 'user_agent', 'kind', 'wifi', 'trans_id', 'device_language',\n",
    "       'wifi_installs', 'last_install', 'first_install', 'session_user_agent'], inplace=True)\n",
    "\n",
    "    modelo = pd.merge(modelo, installs, on='ref_hash', how='outer')\n",
    "    \n",
    "    installs = 0\n",
    "    gc.collect()\n",
    "    \n",
    "    events = pd.read_pickle('data/events_w'+str(window)+'.pkl')\n",
    "    events.drop(columns=['event_uuid', 'ip_address', 'index', 'device_countrycode', 'trans_id'], inplace=True)\n",
    "\n",
    "    events['attributed'] = events['attributed'].map({True: 1, False: 0})\n",
    "    events['wifi'] = events['wifi'].map({True: 1, False: 0})\n",
    "    top_15_events_id = events['event_id'].value_counts().head(15)\n",
    "    events.loc[(~events['event_id'].isin(top_15_events_id.index))&(events['event_id'].notnull()), 'event_id'] = 'Other'\n",
    "    top_20_application_id = events['application_id'].value_counts().head(20)\n",
    "    events.loc[(~events['application_id'].isin(top_20_application_id.index))&(events['application_id'].notnull()), 'application_id'] = 'Other'\n",
    "    #top_15_device_os_version = events['device_os_version'].value_counts().head(15)\n",
    "    #events.loc[(~events['device_os_version'].isin(top_15_device_os_version.index))&(events['device_os_version'].notnull()), 'device_os_version'] = 'Other'\n",
    "    #top_15_device_brand = events['device_brand'].value_counts().head(15)\n",
    "    #events.loc[(~events['device_brand'].isin(top_15_device_brand.index))&(events['device_brand'].notnull()), 'device_brand'] = 'Other'\n",
    "    #top_15_device_model = events['device_model'].value_counts().head(15)\n",
    "    #events.loc[(~events['device_model'].isin(top_15_device_model.index))&(events['device_model'].notnull()), 'device_model'] = 'Other'\n",
    "    top_10_session_user_agent = events['session_user_agent'].value_counts().head(10)\n",
    "    events.loc[(~events['session_user_agent'].isin(top_10_session_user_agent.index))&(events['session_user_agent'].notnull()), 'session_user_agent'] = 'Other'\n",
    "    #top_10_carrier = events['carrier'].value_counts().head(10)\n",
    "    #events.loc[(~events['carrier'].isin(top_10_carrier.index))&(events['carrier'].notnull()), 'carrier'] = 'Other'\n",
    "    top_15_kind = events['kind'].value_counts().head(15)\n",
    "    events.loc[(~events['kind'].isin(top_15_kind.index))&(events['kind'].notnull()), 'kind'] = 'Other'\n",
    "    #top_15_device_language = events['device_language'].value_counts().head(15)\n",
    "    #events.loc[(~events['device_language'].isin(top_15_device_language.index))&(events['device_language'].notnull()), 'device_language'] = 'Other'\n",
    "\n",
    "    gb = events.groupby('ref_hash')\n",
    "    events = events.join(gb['event_id'].value_counts().unstack().add_prefix('event_id_'), on='ref_hash')\n",
    "    events = events.join(gb['application_id'].value_counts().unstack().add_prefix('application_id_'), on='ref_hash')\n",
    "    #events = events.join(gb['device_os_version'].value_counts().unstack().add_prefix('device_os_version_'), on='ref_hash')\n",
    "    #events = events.join(gb['device_brand'].value_counts().unstack().add_prefix('device_brand_'), on='ref_hash')\n",
    "    #events = events.join(gb['device_model'].value_counts().unstack().add_prefix('device_model_'), on='ref_hash')\n",
    "    #events = events.join(gb['carrier'].value_counts().unstack().add_prefix('carrier_'), on='ref_hash')\n",
    "    events = events.join(gb['session_user_agent'].value_counts().unstack().add_prefix('session_user_agent_'), on='ref_hash')\n",
    "    events = events.join(gb['kind'].value_counts().unstack().add_prefix('kind_'), on='ref_hash')\n",
    "    #events = events.join(gb['device_language'].value_counts().unstack().add_prefix('device_language_'), on='ref_hash')\n",
    "\n",
    "    events['n_events'] = gb['date'].transform('count')\n",
    "    events['attributed_events_mean'] = gb['attributed'].transform('mean')\n",
    "    events['wifi_events_mean'] = gb['wifi'].transform('mean')\n",
    "    events['first_event'] = gb['date'].transform('min')\n",
    "    events['last_event'] = gb['date'].transform('max')\n",
    "    events['diff_events'] = (events['last_event'] - events['first_event']).dt.total_seconds()\n",
    "    events['mean_time_events'] = 0\n",
    "    events.loc[events['n_events'] > 1, 'mean_time_events'] = ((events['last_event'] - events['first_event'])/ \\\n",
    "                                     (events['n_events'] -1)).dt.total_seconds()\n",
    "    events['first_event_sec'] = (events['first_event'] - pd.Timestamp(time_windows[window-1][0])).dt.total_seconds()\n",
    "    events['last_event_sec'] = (events['last_event'] - pd.Timestamp(time_windows[window-1][0])).dt.total_seconds()\n",
    "    \n",
    "    events.drop_duplicates(subset='ref_hash', inplace=True)\n",
    "    events = events.drop(columns=['date', 'event_id', 'ref_type', 'application_id', 'attributed', 'device_os_version', 'device_brand', 'device_model', \n",
    "       'device_city', 'session_user_agent', 'user_agent', 'carrier', 'kind', 'device_os', 'wifi', 'connection_type', 'device_language', 'first_event', 'last_event'])\n",
    "    \n",
    "    modelo = pd.merge(modelo, events, on='ref_hash', how='outer')\n",
    "    events = 0\n",
    "    gc.collect()\n",
    "    return modelo\n",
    "\n",
    "def imputacionValores(df):\n",
    "    df['last_auction_sec_to_end'].fillna(24*3600*3, inplace=True)\n",
    "\n",
    "    features = ['diff_auctions', 'mean_time_auction', 'diff_clicks',\n",
    "                'mean_time_click', 'timeToClick_mean', 'touchX_mean', \n",
    "                'touchY_mean', 'latitude_mean', 'longitude_mean',\n",
    "                'wifi_installs_mean', 'attributed_installs_mean', 'diff_installs',\n",
    "                'mean_time_install', 'time_appearence', 'time_appearence_install', \n",
    "                'attributed_events_mean', 'wifi_events_mean', 'diff_events',\n",
    "                'mean_time_events']\n",
    "\n",
    "    for feature in features:\n",
    "        if feature not in df:\n",
    "            continue\n",
    "        else:\n",
    "            df[feature] = df[feature].fillna(df[feature].mean())\n",
    "\n",
    "    for feature in df.columns:\n",
    "        df[feature].fillna(0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 4/4 [20:01<00:00, 299.15s/it]\n"
     ]
    }
   ],
   "source": [
    "model = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(1,5)):\n",
    "    modelo = armadoFeatures(i)\n",
    "\n",
    "    if window <= 4:\n",
    "        auctions_label = pd.read_pickle('data/auctions_w'+str(window)+'_label.pkl')\n",
    "        installs_label = pd.read_pickle('data/installs_w'+str(window)+'_label.pkl')\n",
    "        modelo = pd.merge(modelo, auctions_label, on='ref_hash', how='outer')\n",
    "        modelo = pd.merge(modelo, installs_label, on='ref_hash', how='outer')\n",
    "        modelo['time_appearence'] = modelo['time_appearence'].fillna(max_time)\n",
    "        modelo['time_appearence_install'] = modelo['time_appearence_install'].fillna(max_time)\n",
    "\n",
    "    model = pd.concat([model, modelo], sort=False)\n",
    "\n",
    "    modelo = 0\n",
    "    gc.collect()\n",
    "    \n",
    "#Imputacion valores\n",
    "model = imputacionValores(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = ['time_appearence', 'time_appearence_install']\n",
    "train, test = train_test_split(model.drop(columns=['ref_hash']), test_size=0.2)\n",
    "\n",
    "train_Y = train[target_features[objective]]\n",
    "train_X = train.drop(columns=target_features)\n",
    "test_Y = test[target_features[objective]]\n",
    "test_X = test.drop(columns=target_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMModel(boosting_type=\"gbdt\", num_leaves=70, max_depth=None, learning_rate=0.025, n_estimators=500, \n",
    "                      max_bin=500, subsample_for_bin=50000, objective='regression', min_split_gain=0, min_child_weight=5, \n",
    "                      min_child_samples=10, subsample=1, subsample_freq=1, colsample_bytree=1, reg_alpha=0, reg_lambda=0, \n",
    "                      seed=0, silent=True, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMModel(boosting_type='gbdt',\n",
       "     categoricals=['ref_type_id', 'source_id', 'carrier_id', 'agent_device', 'os_minor', 'os_major', 'specs_brand', 'brand', 'ref_type_x', 'ref_type_y', 'device_brand', 'device_model', 'session_user_agent', 'device_os'],\n",
       "     class_weight=None, colsample_bytree=1, importance_type='split',\n",
       "     learning_rate=0.025, max_bin=500, max_depth=None,\n",
       "     min_child_samples=10, min_child_weight=5, min_split_gain=0,\n",
       "     n_estimators=500, n_jobs=-1, num_leaves=70, objective='regression',\n",
       "     random_state=None, reg_alpha=0, reg_lambda=0, seed=0, silent=True,\n",
       "     subsample=1, subsample_for_bin=50000, subsample_freq=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69173.11044756648"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_train = np.sqrt(mean_squared_error(lgb_model.predict(train_X), train_Y))\n",
    "error_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69219.4417460478"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_test = np.sqrt(mean_squared_error(lgb_model.predict(test_X), test_Y))\n",
    "error_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('lgb1_data_final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('nuevos_modelos/lgb1_data_final.pkl')\n",
    "target = pd.read_csv('data/target_competencia_ids.csv')\n",
    "target['ref_hash'] = target['ref_hash'].apply(lambda x: int(str(x)[:-3]))\n",
    "target = target.drop_duplicates('ref_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4037, 176)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicciones auctions:\n",
    "lgb_model_auctions = pickle.load(open('nuevos_modelos/lgb1_all_auctions', 'rb'))\n",
    "\n",
    "pred_data_auctions = lgb_model_auctions.predict(data.drop(columns=['ref_hash']))\n",
    "predictions_auctions = pd.DataFrame({'ref_hash': data['ref_hash'],'obj': pred_data_auctions})\n",
    "\n",
    "predictions_auctions['ref_hash'] = predictions_auctions['ref_hash'].apply(lambda x: str(x) + '_st')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicciones installs:\n",
    "lgb_model_installs = pickle.load(open('nuevos_modelos/lgb1_all_installs', 'rb'))\n",
    "\n",
    "pred_data_installs = lgb_model_installs.predict(data.drop(columns=['ref_hash']))\n",
    "predictions_installs = pd.DataFrame({'ref_hash': data['ref_hash'],'obj': pred_data_installs})\n",
    "\n",
    "predictions_installs['ref_hash'] = predictions_installs['ref_hash'].apply(lambda x: str(x) + '_sc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_auctions.loc[round(predictions_auctions['obj']) == 189823, 'obj'] = (189823.284861+predictions_auctions['obj'].mean())/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat([predictions_auctions, predictions_installs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref_hash</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8027109759910869730_st</td>\n",
       "      <td>161952.365447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3805512975348983658_st</td>\n",
       "      <td>108786.033585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>706875581985023190_st</td>\n",
       "      <td>64673.774385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9201763056911976665_st</td>\n",
       "      <td>27848.106768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2070001883938629880_st</td>\n",
       "      <td>46166.708285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2956299000597738624_st</td>\n",
       "      <td>2726.349251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5051062186658844309_st</td>\n",
       "      <td>41323.710753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3729857814892336524_st</td>\n",
       "      <td>43936.008958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8048087799114816623_st</td>\n",
       "      <td>122001.951295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7988921706433140919_st</td>\n",
       "      <td>37892.576905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ref_hash            obj\n",
       "0  8027109759910869730_st  161952.365447\n",
       "1  3805512975348983658_st  108786.033585\n",
       "2   706875581985023190_st   64673.774385\n",
       "3  9201763056911976665_st   27848.106768\n",
       "4  2070001883938629880_st   46166.708285\n",
       "5  2956299000597738624_st    2726.349251\n",
       "6  5051062186658844309_st   41323.710753\n",
       "7  3729857814892336524_st   43936.008958\n",
       "8  8048087799114816623_st  122001.951295\n",
       "9  7988921706433140919_st   37892.576905"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('predicciones/predicciones19.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
